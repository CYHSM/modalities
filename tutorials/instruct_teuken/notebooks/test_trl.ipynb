{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42163439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"\n",
    "import wandb\n",
    "import torch\n",
    "import json\n",
    "import subprocess\n",
    "import time\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainerCallback\n",
    "from datasets import load_dataset\n",
    "from trl import SFTConfig, SFTTrainer, setup_chat_format\n",
    "from lighteval.pipeline import Pipeline, PipelineParameters, ParallelismManager\n",
    "from lighteval.logging.evaluation_tracker import EvaluationTracker\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8242cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "# login()\n",
    "wandb.init(project=\"trl-Teuken3.73T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03183f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "finetune_name = \"Teuken-Instruct-TRL\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d04025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"/raid/s3/opengptx/mfrey/3.73T-Tokens/checkpoints/aug15_tokfix\", trust_remote_code=True, device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/raid/s3/opengptx/mfrey/3.73T-Tokens/checkpoints/aug15_tokfix\")\n",
    "model, tokenizer = setup_chat_format(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43f27b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export HF_HOME=\"/raid/s3/opengptx/mfrey/huggingface\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f3be32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and format Alpaca dataset\n",
    "# ds = load_dataset(\"tatsu-lab/alpaca\", split=\"train\")\n",
    "ds = load_dataset(\"meta-math/MetaMathQA\", split=\"train\")\n",
    "\n",
    "def format_alpaca(example):\n",
    "    \"\"\"Convert Alpaca format to chat format\"\"\"\n",
    "    if example[\"input\"]:\n",
    "        content = f\"{example['instruction']}\\n\\nInput: {example['input']}\"\n",
    "    else:\n",
    "        content = example[\"instruction\"]\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": content},\n",
    "            {\"role\": \"assistant\", \"content\": example[\"output\"]}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "def format_metamath(example):\n",
    "    \"\"\"Format MetaMath dataset to chat format\"\"\"\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": example[\"query\"]},\n",
    "            {\"role\": \"assistant\", \"content\": example[\"response\"]}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "ds = ds.map(format_metamath, remove_columns=ds.column_names)\n",
    "ds = ds.train_test_split(test_size=0.01, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824d65c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lighteval_cli_async(checkpoint_path, step, gpu_id=6, max_samples=10):\n",
    "    \"\"\"\n",
    "    Run LightEval CLI with quoted arguments, save results in the checkpoint folder,\n",
    "    and return the results.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"üîç Starting CLI evaluation for step {step} on {checkpoint_path}\")\n",
    "        \n",
    "        checkpoint_dir = Path(checkpoint_path)\n",
    "        \n",
    "        # Define the argument strings\n",
    "        model_args = f\"model_name={checkpoint_path},use_chat_template=True,trust_remote_code=True,batch_size=16\"\n",
    "        tasks = \"leaderboard|hellaswag|0|1,leaderboard|gsm8k|0|1\"\n",
    "        \n",
    "        # Construct the full command as a single string, with quotes around the arguments\n",
    "        cmd_string = (\n",
    "            f'lighteval accelerate '\n",
    "            f'\"{model_args}\" '\n",
    "            f'\"{tasks}\" '\n",
    "            f'--max-samples {max_samples} '\n",
    "        )\n",
    "        \n",
    "        # Set environment variables\n",
    "        env = os.environ.copy()\n",
    "        env[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n",
    "        env[\"HF_HOME\"] = \"/raid/s3/opengptx/mfrey/huggingface\"\n",
    "        \n",
    "        print(f\"üöÄ Running command: CUDA_VISIBLE_DEVICES={gpu_id} {cmd_string}\")\n",
    "        \n",
    "        # Run the evaluation using shell=True to correctly interpret the quotes\n",
    "        result = subprocess.run(\n",
    "            cmd_string,\n",
    "            shell=True,  # Use the shell to parse the command string\n",
    "            env=env,\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            check=False,\n",
    "            cwd=os.getcwd()\n",
    "        )\n",
    "        \n",
    "        if result.returncode != 0:\n",
    "            print(f\"‚ùå Evaluation failed for step {step}\")\n",
    "            print(f\"STDOUT: {result.stdout}\")\n",
    "            print(f\"STDERR: {result.stderr}\")\n",
    "            return None\n",
    "            \n",
    "        print(f\"‚úÖ CLI evaluation completed for step {step}\")\n",
    "        \n",
    "        # Find the results JSON file in the checkpoint directory\n",
    "        json_files = list(checkpoint_dir.glob(\"results_*.json\"))\n",
    "        if not json_files:\n",
    "            print(f\"‚ùå No results JSON file found in {checkpoint_dir}\")\n",
    "            return None\n",
    "            \n",
    "        # Read the most recent results file\n",
    "        results_file = max(json_files, key=lambda p: p.stat().st_mtime)\n",
    "        print(f\"üìñ Reading results from {results_file}\")\n",
    "        \n",
    "        with open(results_file, 'r') as f:\n",
    "            eval_results = json.load(f)\n",
    "            \n",
    "        return eval_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Evaluation failed for step {step}: {e}\")\n",
    "        return None\n",
    "\n",
    "def parse_and_log_results(eval_results, step):\n",
    "    \"\"\"Parse LightEval results and log to WandB\"\"\"\n",
    "    if not eval_results or \"results\" not in eval_results:\n",
    "        print(f\"‚ùå No valid results to log for step {step}\")\n",
    "        return\n",
    "        \n",
    "    results_to_log = {}\n",
    "    \n",
    "    # Parse individual task results\n",
    "    for task_name, metrics in eval_results[\"results\"].items():\n",
    "        if task_name == \"all\":  # Skip the aggregated results\n",
    "            continue\n",
    "            \n",
    "        # Clean up task name for logging\n",
    "        clean_task_name = task_name.replace(\"leaderboard|\", \"\").split(\"|\")[0]\n",
    "        \n",
    "        for metric_name, value in metrics.items():\n",
    "            if not metric_name.endswith(\"_stderr\"):  # Skip stderr metrics\n",
    "                log_key = f\"eval/{clean_task_name}_{metric_name}\"\n",
    "                results_to_log[log_key] = value\n",
    "                \n",
    "    # Also log some metadata\n",
    "    if \"config_general\" in eval_results:\n",
    "        config = eval_results[\"config_general\"]\n",
    "        if \"total_evaluation_time_secondes\" in config:\n",
    "            results_to_log[\"eval/evaluation_time_seconds\"] = float(config[\"total_evaluation_time_secondes\"])\n",
    "        if \"model_size\" in config:\n",
    "            results_to_log[\"eval/model_size\"] = config[\"model_size\"]\n",
    "            \n",
    "    # Log to WandB\n",
    "    if results_to_log:\n",
    "        wandb.log(results_to_log, step=step)\n",
    "        print(f\"üìä Logged {len(results_to_log)} metrics to WandB for step {step}\")\n",
    "        for key, value in results_to_log.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "    else:\n",
    "        print(f\"‚ùå No metrics to log for step {step}\")\n",
    "\n",
    "class EvalCallback(TrainerCallback):\n",
    "    \"\"\"Callback to trigger async LightEval CLI on checkpoint saves\"\"\"\n",
    "    def __init__(self, gpu_id=6, max_samples=10, max_workers=2):\n",
    "        self.gpu_id = gpu_id\n",
    "        self.max_samples = max_samples\n",
    "        self.executor = ThreadPoolExecutor(max_workers=max_workers)\n",
    "        self.futures = []\n",
    "    \n",
    "    def on_save(self, args, state, control, **kwargs):\n",
    "        \"\"\"Trigger evaluation when checkpoint is saved\"\"\"\n",
    "        checkpoint_path = os.path.join(args.output_dir, f\"checkpoint-{state.global_step}\")\n",
    "        \n",
    "        # Check if checkpoint directory exists\n",
    "        if not os.path.exists(checkpoint_path):\n",
    "            print(f\"‚ö†Ô∏è  Checkpoint path {checkpoint_path} does not exist, skipping evaluation\")\n",
    "            return\n",
    "            \n",
    "        print(f\"üíæ Checkpoint saved at step {state.global_step}, triggering evaluation...\")\n",
    "        \n",
    "        # Submit evaluation job\n",
    "        def eval_and_log():\n",
    "            results = run_lighteval_cli_async(checkpoint_path, state.global_step, self.gpu_id, self.max_samples)\n",
    "            if results:\n",
    "                parse_and_log_results(results, state.global_step)\n",
    "        \n",
    "        future = self.executor.submit(eval_and_log)\n",
    "        self.futures.append(future)\n",
    "        \n",
    "        # Clean up completed futures\n",
    "        self.futures = [f for f in self.futures if not f.done()]\n",
    "        \n",
    "        print(f\"üéØ Evaluation job submitted for step {state.global_step}\")\n",
    "    \n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        \"\"\"Wait for all evaluations to complete\"\"\"\n",
    "        if self.futures:\n",
    "            print(\"‚è≥ Waiting for remaining evaluations to complete...\")\n",
    "            for future in self.futures:\n",
    "                try:\n",
    "                    future.result(timeout=300)  # 5 minute timeout per evaluation\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Evaluation future failed: {e}\")\n",
    "        \n",
    "        self.executor.shutdown(wait=True)\n",
    "        print(\"‚úÖ All evaluations completed\")\n",
    "\n",
    "SOURCE_MODEL_PATH = \"/raid/s3/opengptx/mfrey/3.73T-Tokens/checkpoints/aug15_tokfix\"\n",
    "class CustomSFTTrainer(SFTTrainer):\n",
    "    def __init__(self, source_model_path, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.source_model_path = source_model_path\n",
    "    \n",
    "    def save_model(self, output_dir=None, _internal_call=False):\n",
    "        super().save_model(output_dir, _internal_call)\n",
    "        if output_dir is None:\n",
    "            output_dir = self.args.output_dir\n",
    "            \n",
    "        save_model_with_custom_code(self.model, output_dir, self.source_model_path)\n",
    "\n",
    "def save_model_with_custom_code(model, save_path, source_model_path):\n",
    "    \"\"\"Save model and copy custom code files\"\"\"\n",
    "    import shutil\n",
    "    from pathlib import Path\n",
    "    if hasattr(model, 'save_pretrained'):\n",
    "        model.save_pretrained(save_path)\n",
    "    source_path = Path(source_model_path)\n",
    "    save_path = Path(save_path)\n",
    "    \n",
    "    custom_files = [\"modeling_gpt2.py\", \"configuration_gpt2.py\"]\n",
    "    \n",
    "    for file_name in custom_files:\n",
    "        source_file = source_path / file_name\n",
    "        dest_file = save_path / file_name\n",
    "        \n",
    "        if source_file.exists():\n",
    "            shutil.copy(source_file, dest_file)\n",
    "            print(f\"‚úÖ Copied {file_name} to {save_path}\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  Warning: {file_name} not found in {source_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3046f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure SFTTrainer with W&B logging\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=\"/raid/s3/opengptx/mfrey/3.73T-Tokens/checkpoints/aug15_tokfix/instruct\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=1e-5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    save_steps=1000,\n",
    "    eval_steps=1000,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    report_to=\"wandb\",\n",
    "    gradient_checkpointing=True,\n",
    "    # Mixed precision training\n",
    "    #fp16=torch.cuda.is_available(),\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00331e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = CustomSFTTrainer(\n",
    "    source_model_path=SOURCE_MODEL_PATH,\n",
    "    model=model,\n",
    "    args=sft_config,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    eval_dataset=ds[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    callbacks=[EvalCallback(gpu_id=7, max_samples=500)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b741b878",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train the model\n",
    "print(\"üöÄ Starting training...\")\n",
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d95e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close W&B run\n",
    "wandb.finish()\n",
    "\n",
    "print(\"‚ú® Training and evaluation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b0e701",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a56f77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modalities",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
